# -*- coding: utf-8 -*-
"""Tugas Project Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xhwBCFkE65mb5EdHi_-QO5gtulPhR4sd

# ***Logistic Regression Prediction on Titanic Dataset***
"""

# Importing important Libraries
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, mean_squared_error, mean_absolute_error, f1_score

"""# ***Get Dataset***"""

# Getting Dataset (But first you have to upload heart.csv into google colab files)
heart_data = pd.read_csv("heart.csv")

"""# ***View head data in dataset***"""

# Print the first 5 rows of each variables
print(heart_data.head())

# Checking data types of all variables
heart_data.dtypes

"""Description of each column:

age - age in years

sex - (1 = male; 0 = female)

cp - chest pain type

trestbps - resting blood pressure (in mm Hg on admission to the 
hospital)

chol - serum cholestoral in mg/dl

fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)

restecg - resting electrocardiographic results

thalach - maximum heart rate achieved

exang - exercise induced angina (1 = yes; 0 = no)

oldpeak - ST depression induced by exercise relative to rest

slope - the slope of the peak exercise ST segment

ca - number of major vessels (0-3) colored by flourosopy

thal - 3 = normal; 6 = fixed defect; 7 = reversable defect

target - have disease or not (1=yes, 0=no)

# Checking Null Value
"""

# Showing each variables in the dataset if there's any null/NaN values
heart_data.isna()

# Check how many null value
heart_data.isna().sum()

# Visualize any null value
sb.heatmap(heart_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')
# Dari dataset yang kita dapatkan tidak ditemukan null value jadi tidak perlu melakukan data fixing

"""# Find Out How Many Target Have Disease"""

# Checking the distribution result of independent variables
sb.countplot(x='target', data = heart_data)

"""# Male vs Female Chance of Getting Heart Disease"""

# Checking the rate of getting heart disease by gender data
sb.countplot(x='target', data = heart_data, hue = 'sex')

"""# What Attributes That Have Only 2 Unique Values"""

# Checking number of unique value in every column
heartDf = pd.DataFrame(heart_data)
ageunique = len(pd.unique(heartDf['age']))
sexunique = len(pd.unique(heartDf['sex']))
cpunique = len(pd.unique(heartDf['cp']))
trestbpsunique = len(pd.unique(heartDf['trestbps']))
cholunique = len(pd.unique(heartDf['chol']))
fbsunique = len(pd.unique(heartDf['fbs']))
restecgunique = len(pd.unique(heartDf['restecg']))
thalachunique = len(pd.unique(heartDf['thalach']))
exangunique = len(pd.unique(heartDf['exang']))
oldpeakunique = len(pd.unique(heartDf['oldpeak']))
slopeunique = len(pd.unique(heartDf['slope']))
caunique = len(pd.unique(heartDf['ca']))
thalunique = len(pd.unique(heartDf['thal']))
targetunique = len(pd.unique(heartDf['target']))
print("Number of Unique: ")
print("Age: ",ageunique)
print("Sex: ",sexunique)
print("Cp: ",cpunique)
print("trestbps: ",trestbpsunique)
print("chol: ",cholunique)
print("fbs: ",fbsunique)
print("restecg: ",restecgunique)
print("thalach: ",thalachunique)
print("exang: ",exangunique)
print("oldpeak: ",oldpeakunique)
print("slope: ",slopeunique)
print("ca: ",caunique)
print("thal: ",thalunique)
print("target: ",targetunique)

"""# Drop Column That Are Not Required in Prediction"""

# Removing column that dont have big influence into prediction
heart_data.drop(['sex','fbs','exang'],axis=1,inplace=True)
heart_data.dtypes

"""# Checking Correlation Between Each Variable"""

# Check how strong relation between each variables and to determine Independent and Dependant Variables
Correlation = heart_data.corr()
sb.heatmap(Correlation, xticklabels=Correlation.columns,yticklabels=Correlation.columns, annot=True)

"""# Separating Dependent and Independent Variables"""

x = heart_data[['age', 'cp', 'trestbps', 'chol', 'restecg', 'thalach', 'oldpeak', 'slope', 'ca', 'thal']]
y = heart_data[['target']]
print(x,y)

"""# Logistic Regression Modelling"""

# Splitting train and test data from the beginning dataset
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)

# Making Logistic Regression Model
LogisticR = LogisticRegression(C = 0.01, solver = 'liblinear', random_state=0)
LogisticR.fit(X_train, Y_train)

# Predicting Logistic Regression Using X_test
Predict = LogisticR.predict(X_test)

"""# Testing Logistic Regression"""

# Measuring performance from Logistic Regression
accuracy = accuracy_score(Y_test, Predict)
MAE = mean_absolute_error(Y_test, Predict)
MSE = mean_squared_error(Y_test, Predict)
F1 = f1_score(Y_test, Predict, average='weighted')

print(" Accuration : %.2f" % accuracy)
print(" Mean Absolute Error : %.2f" % MAE)
print(" Mean Squared Error : %.2f" % MSE)
print(" F1 : %.2f" % F1)

# Print Prediction Result and Expected Result
print(Predict)
print(np.array(Y_test))

"""# Evaluate The Model and Experimental Result Analysis"""

# Visualize incorrect and correct prediction based on factual result
cnf_matrix = confusion_matrix(Y_test, Predict, labels=[0,1])
pd.DataFrame(cnf_matrix,columns=['Predicted Don\'t Have Disease','Predicted Have Disease'],index=['Actual Don\'t Have Disease','Actual Have Disease'])

# Classification Report after Logistic Regression Prediction
print(classification_report(Y_test,Predict))